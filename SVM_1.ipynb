{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1SCJ4T2ayo4uiMRvlvo85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkmlworld/Master_DS_ineuron/blob/main/SVM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7GKIVzezH-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. How to explain SVM in interview?**"
      ],
      "metadata": {
        "id": "agWRSUZgzKg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining Support Vector Machines (SVM) in an interview requires breaking down the concept into understandable parts. Here's a concise and clear way to explain SVM:\n",
        "\n",
        "Introduction:\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for classification tasks. Its main objective is to find the optimal hyperplane that best separates different classes in the feature space.\n",
        "\n",
        "Basic Concept:\n",
        "SVM works by mapping input data points into a high-dimensional feature space where it attempts to find the hyperplane that best divides the classes. This hyperplane is chosen in such a way that it maximizes the margin, which is the distance between the hyperplane and the closest data points from each class, known as support vectors.\n",
        "\n",
        "Margin:\n",
        "The margin is crucial in SVM because it ensures better generalization to unseen data. A larger margin implies better generalization as it allows for more flexibility before encountering misclassifications.\n",
        "\n",
        "Kernel Trick:\n",
        "SVM can efficiently handle non-linearly separable data using the kernel trick. Instead of explicitly mapping data into a higher-dimensional space, SVM computes the dot product between data points in the original space and applies a kernel function to obtain the same effect. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
        "\n",
        "Optimization Objective:\n",
        "SVM aims to minimize the classification error while maximizing the margin. This objective is achieved through optimization techniques like gradient descent or quadratic programming.\n",
        "\n",
        "Categorical vs. Binary Classification:\n",
        "While SVM is commonly associated with binary classification, it can be extended to handle multi-class classification tasks using strategies like one-vs-one or one-vs-all.\n",
        "\n",
        "Regularization Parameter (C):\n",
        "The regularization parameter, denoted as C, balances the trade-off between maximizing the margin and minimizing classification errors. A smaller C allows for a wider margin but may lead to more misclassifications, while a larger C penalizes misclassifications more heavily, potentially leading to a narrower margin.\n",
        "\n",
        "Key Advantages:\n",
        "\n",
        "Effective in high-dimensional spaces.\n",
        "Versatile due to the kernel trick for handling non-linear decision boundaries.\n",
        "Memory efficient as it only uses a subset of training points (support vectors) to define the decision boundary.\n",
        "Limitations:\n",
        "\n",
        "SVM might not perform well with large datasets due to its computational complexity.\n",
        "Selection of the appropriate kernel and its parameters can be challenging.\n",
        "Interpretability of the model might be limited compared to simpler models like logistic regression.\n",
        "By covering these points, you can provide a comprehensive understanding of SVM in an interview setting. Additionally, it's helpful to illustrate with diagrams or examples to make the explanation more intuitive.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "peJt2lDNz02u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. What is the mathematical formula for a linear SVM?**"
      ],
      "metadata": {
        "id": "l7CygbuNz56d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical formulation for a linear Support Vector Machine (SVM) involves defining the decision boundary (hyperplane) that maximally separates the classes. Here's the basic mathematical formulation:\n",
        "\n",
        "Given a dataset with\n",
        "�\n",
        "m samples and\n",
        "�\n",
        "n features, where\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "x\n",
        "(i)\n",
        "  represents the\n",
        "�\n",
        "i-th feature vector and\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  represents the corresponding class label, with\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "∈\n",
        "{\n",
        "−\n",
        "1\n",
        ",\n",
        "1\n",
        "}\n",
        "y\n",
        "(i)\n",
        " ∈{−1,1}.\n",
        "\n",
        "Decision Function:\n",
        "The decision function for a linear SVM is defined as:\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "=\n",
        "w\n",
        "�\n",
        "⋅\n",
        "x\n",
        "+\n",
        "�\n",
        "f(x)=w\n",
        "T\n",
        " ⋅x+b\n",
        "where:\n",
        "\n",
        "w\n",
        "w is the weight vector.\n",
        "x\n",
        "x is the input feature vector.\n",
        "�\n",
        "b is the bias term.\n",
        "Optimization Objective:\n",
        "The objective of linear SVM is to find the hyperplane with the maximum margin, which can be formulated as an optimization problem. This is typically expressed as minimizing:\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "2\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to the constraints:\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "(\n",
        "w\n",
        "�\n",
        "⋅\n",
        "x\n",
        "(\n",
        "�\n",
        ")\n",
        "+\n",
        "�\n",
        ")\n",
        "≥\n",
        "1\n",
        "for\n",
        "�\n",
        "=\n",
        "1\n",
        ",\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "�\n",
        "y\n",
        "(i)\n",
        " (w\n",
        "T\n",
        " ⋅x\n",
        "(i)\n",
        " +b)≥1for i=1,2,...,m\n",
        "These constraints ensure that data points are correctly classified and are at least marginally on the correct side of the decision boundary.\n",
        "\n",
        "Objective Function:\n",
        "The objective function to be minimized is the square of the Euclidean norm of the weight vector:\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "2\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "This term is halved for mathematical convenience as it simplifies the derivative during optimization.\n",
        "\n",
        "Margin:\n",
        "The margin can be calculated as\n",
        "2\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "∣∣w∣∣\n",
        "2\n",
        "​\n",
        " . Maximizing the margin is equivalent to minimizing\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "∣∣w∣∣.\n",
        "\n",
        "Optimization:\n",
        "Linear SVM optimization can be solved using techniques like gradient descent or quadratic programming to find the optimal values for\n",
        "w\n",
        "w and\n",
        "�\n",
        "b that satisfy the constraints and minimize the objective function.\n",
        "\n",
        "This formulation seeks to find the optimal hyperplane that separates the classes with the maximum margin, allowing for robust classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qtfIoy37z59I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. What is the objective function of a linear SVM?**"
      ],
      "metadata": {
        "id": "FnH5dmhRz5_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective function of a linear Support Vector Machine (SVM) is typically formulated to maximize the margin between the classes while minimizing classification errors. In a linear SVM, the objective function is expressed as:\n",
        "\n",
        "min\n",
        "⁡\n",
        "w\n",
        ",\n",
        "�\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "2\n",
        "min\n",
        "w,b\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "\n",
        "Subject to the constraints:\n",
        "\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "(\n",
        "w\n",
        "�\n",
        "⋅\n",
        "x\n",
        "(\n",
        "�\n",
        ")\n",
        "+\n",
        "�\n",
        ")\n",
        "≥\n",
        "1\n",
        "y\n",
        "(i)\n",
        " (w\n",
        "T\n",
        " ⋅x\n",
        "(i)\n",
        " +b)≥1\n",
        "\n",
        "for\n",
        "�\n",
        "=\n",
        "1\n",
        ",\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "�\n",
        "i=1,2,...,m\n",
        "\n",
        "where:\n",
        "\n",
        "w\n",
        "w is the weight vector.\n",
        "�\n",
        "b is the bias term.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "x\n",
        "(i)\n",
        "  represents the\n",
        "�\n",
        "i-th feature vector.\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  represents the corresponding class label, with\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "∈\n",
        "{\n",
        "−\n",
        "1\n",
        ",\n",
        "1\n",
        "}\n",
        "y\n",
        "(i)\n",
        " ∈{−1,1}.\n",
        "�\n",
        "m is the number of samples in the dataset.\n",
        "The objective function seeks to minimize the square of the Euclidean norm of the weight vector\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "w\n",
        "∣\n",
        "∣\n",
        "2\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " . This term is halved for mathematical convenience as it simplifies the derivative during optimization.\n",
        "\n",
        "The constraints\n",
        "�\n",
        "(\n",
        "�\n",
        ")\n",
        "(\n",
        "w\n",
        "�\n",
        "⋅\n",
        "x\n",
        "(\n",
        "�\n",
        ")\n",
        "+\n",
        "�\n",
        ")\n",
        "≥\n",
        "1\n",
        "y\n",
        "(i)\n",
        " (w\n",
        "T\n",
        " ⋅x\n",
        "(i)\n",
        " +b)≥1 ensure that each data point is correctly classified and lies on the correct side of the decision boundary. These constraints are crucial for maximizing the margin between the classes.\n",
        "\n",
        "By minimizing the objective function while satisfying these constraints, the linear SVM finds the optimal hyperplane that separates the classes with the maximum margin, leading to robust classification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-dAj7lAz6Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. What is the kernel trick in SVM?**"
      ],
      "metadata": {
        "id": "5lTwnodH0fcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kernel trick is a technique used in Support Vector Machines (SVM) to handle non-linear decision boundaries by implicitly mapping input data into a higher-dimensional feature space. It allows SVMs to efficiently classify data that may not be linearly separable in the original input space.\n",
        "\n",
        "Here's how the kernel trick works:\n",
        "\n",
        "Mapping to Higher Dimension: In SVM, the primary goal is to find a hyperplane that separates different classes. In many cases, the classes might not be separable with a linear boundary in the original feature space.\n",
        "\n",
        "Kernel Functions: Instead of explicitly transforming the input data into a higher-dimensional space, the kernel trick computes the dot product between data points in the original space as if they were in the higher-dimensional space. This is done using kernel functions.\n",
        "\n",
        "Kernel Functions: The kernel functions compute the similarity between pairs of data points. Common kernel functions include:\n",
        "\n",
        "Linear Kernel:\n",
        "�\n",
        "(\n",
        "x\n",
        ",\n",
        "y\n",
        ")\n",
        "=\n",
        "x\n",
        "�\n",
        "⋅\n",
        "y\n",
        "K(x,y)=x\n",
        "T\n",
        " ⋅y\n",
        "Polynomial Kernel:\n",
        "�\n",
        "(\n",
        "x\n",
        ",\n",
        "y\n",
        ")\n",
        "=\n",
        "(\n",
        "x\n",
        "�\n",
        "⋅\n",
        "y\n",
        "+\n",
        "�\n",
        ")\n",
        "�\n",
        "K(x,y)=(x\n",
        "T\n",
        " ⋅y+c)\n",
        "d\n",
        "\n",
        "Radial Basis Function (RBF) Kernel:\n",
        "�\n",
        "(\n",
        "x\n",
        ",\n",
        "y\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "∣\n",
        "∣\n",
        "x\n",
        "−\n",
        "y\n",
        "∣\n",
        "∣\n",
        "2\n",
        "2\n",
        "�\n",
        "2\n",
        ")\n",
        "K(x,y)=exp(−\n",
        "2σ\n",
        "2\n",
        "\n",
        "∣∣x−y∣∣\n",
        "2\n",
        "\n",
        "​\n",
        " )\n",
        "Sigmoid Kernel:\n",
        "�\n",
        "(\n",
        "x\n",
        ",\n",
        "y\n",
        ")\n",
        "=\n",
        "tanh\n",
        "⁡\n",
        "(\n",
        "�\n",
        "x\n",
        "�\n",
        "⋅\n",
        "y\n",
        "+\n",
        "�\n",
        ")\n",
        "K(x,y)=tanh(αx\n",
        "T\n",
        " ⋅y+c)\n",
        "Advantages:\n",
        "\n",
        "Avoids the computational burden of explicitly transforming data into higher dimensions.\n",
        "Allows SVM to handle non-linear decision boundaries efficiently.\n",
        "Offers flexibility in choosing appropriate kernel functions based on the problem domain.\n",
        "Optimization: With the kernel trick, the SVM optimization problem only involves computing dot products between data points in the original space, making it computationally efficient.\n",
        "\n",
        "In summary, the kernel trick enables SVMs to effectively handle non-linear classification problems by implicitly operating in a higher-dimensional feature space, leading to more flexible decision boundaries and improved classification performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NUjB4sZc0ffS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. What is the role of support vectors in SVM Explain with example**"
      ],
      "metadata": {
        "id": "S1x_GSTl0fhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Machines (SVM), support vectors play a crucial role in defining the decision boundary. They are the data points that lie closest to the decision boundary, and they determine the position and orientation of the hyperplane. Understanding the role of support vectors is key to understanding how SVM works.\n",
        "\n",
        "Here's an explanation with an example:\n",
        "\n",
        "Imagine you have a dataset consisting of two classes, represented in a two-dimensional space: class A (blue circles) and class B (red squares). You want to find the optimal hyperplane that best separates these two classes.\n",
        "\n",
        "\n",
        "In the above image, the solid line represents the decision boundary (hyperplane) found by the SVM algorithm. The dashed lines represent the margins, which are the distances between the decision boundary and the closest data points from each class.\n",
        "\n",
        "Now, let's identify the support vectors:\n",
        "\n",
        "For class A, the support vectors are the blue circles that lie closest to the decision boundary. In this case, there are three support vectors from class A.\n",
        "For class B, the support vectors are the red squares that lie closest to the decision boundary. In this case, there are two support vectors from class B.\n",
        "These support vectors are crucial because they have the maximum influence on the position and orientation of the decision boundary. The decision boundary is solely determined by these support vectors, and removing any other data points that are not support vectors would not change the decision boundary.\n",
        "\n",
        "In essence, the support vectors define the margin, which is the distance between the decision boundary and the closest data points from each class. Maximizing this margin is the primary objective of SVM because it leads to better generalization and robustness against noise in the data.\n",
        "\n",
        "Therefore, in SVM, the support vectors are the critical data points that define the decision boundary and determine the overall performance of the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gzbiZcJb0fj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?**"
      ],
      "metadata": {
        "id": "KxdL2ysC0fl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's illustrate the concepts of Hyperplane, Margin, Soft Margin, and Hard Margin in Support Vector Machines (SVM) with examples and graphs.\n",
        "\n",
        "1. Hyperplane:\n",
        "The hyperplane is the decision boundary that separates the classes in SVM. In a binary classification problem with two features, the hyperplane is a line. In three dimensions, it's a plane, and so on. Here's an example:\n",
        "\n",
        "Example: Consider a simple 2D dataset with two classes, labeled as red and blue. The hyperplane (line in this case) separates the two classes as shown:\n",
        "\n",
        "\n",
        "2. Margin:\n",
        "The margin is the distance between the hyperplane and the nearest data point from each class. Maximizing the margin leads to better generalization. Here's an example:\n",
        "\n",
        "Example: The dashed lines in the following graph represent the margins around the hyperplane. The data points closest to the hyperplane are the support vectors.\n",
        "\n",
        "\n",
        "3. Soft Margin:\n",
        "In some cases, the data might not be linearly separable, or there could be outliers. Soft margin SVM allows for misclassifications (violations of the margin) to find a better decision boundary. Here's an example:\n",
        "\n",
        "Example: In the presence of outliers, a soft margin SVM allows for some data points to be within the margin or even on the wrong side of the hyperplane to achieve better overall separation:\n",
        "\n",
        "\n",
        "4. Hard Margin:\n",
        "Hard margin SVM, on the other hand, requires that all data points be correctly classified and lie on the correct side of the margin. It's suitable for linearly separable data with no outliers. Here's an example:\n",
        "\n",
        "Example: In this case, the data is perfectly separable, and the hard margin SVM finds a hyperplane that separates the classes without any margin violations:\n",
        "\n",
        "\n",
        "In summary, while the hyperplane separates classes, the margin, soft margin, and hard margin concepts determine the flexibility of the SVM model in handling data and outliers. Soft margin allows for more flexibility in the presence of noise or outliers, while hard margin seeks a perfect separation, suitable for clean, linearly separable data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tarxOIHd2A6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. SVM Implementation through Iris dataset.**\n",
        "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
        "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
        "~ Compute the accuracy of the model on the testing setl\n",
        "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
        "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
        "the model."
      ],
      "metadata": {
        "id": "V9gAdNZa2A9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0FnX_3rf2A_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Bonus task: Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation.*"
      ],
      "metadata": {
        "id": "5nee5Nc52BB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q7Ce_eK62BEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7EMacVSM2BGR"
      }
    }
  ]
}