{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHYvr3FZFWVJ8GXOoHzFBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkmlworld/Master_DS_ineuron/blob/main/iNeuron3rd_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
        "application.**\n"
      ],
      "metadata": {
        "id": "LsTNJQ76ptFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. This transformation preserves the relative differences between the original values while ensuring that all features have the same scale, which can be beneficial for certain machine learning algorithms that are sensitive to the scale of the input features.\n",
        "\n",
        "Here's how Min-Max scaling is applied:\n",
        "\n",
        "Calculate Min and Max Values: Compute the minimum (min) and maximum (max) values of the feature you want to scale across the entire dataset.\n",
        "\n",
        "Scale the Values: For each value in the feature, apply the following formula to rescale it to the range [0, 1]:\n",
        "\n",
        "X_scaled = (X - X_min) / (X_max - X_min)\n"
      ],
      "metadata": {
        "id": "FzPpb8e9qvY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example to illustrate the application of Min-Max scaling:\n",
        "\n",
        "Suppose we have a dataset with a numerical feature representing the age of individuals, ranging from 20 to 60 years. We want to scale this feature using Min-Max scaling.\n",
        "\n",
        "Original dataset:\n",
        "Age\n",
        "20\n",
        "30\n",
        "40\n",
        "50\n",
        "60\n",
        "Calculate the min and max values:\n",
        "\n",
        "min(Age) = 20\n",
        "max(Age) = 60\n",
        "Scale the values using the Min-Max scaling formula:\n",
        "Age_scaled = (Age - 20) / (60 - 20)\n",
        "          = (Age - 20) / 40\n",
        "Calculating the scaled values:\n",
        "\n",
        "\n",
        "\n",
        "For Age = 20:\n",
        "            \n",
        "            Age_scaled\n",
        "            \n",
        "            = (20 - 20) / 40\n",
        "\n",
        "           = 0 / 40\n",
        "\n",
        "           = 0\n",
        "\n",
        "For Age = 30:\n",
        "\n",
        "Age_scaled = (30 - 20) / 40\n",
        "\n",
        "\n",
        "           = 10 / 40\n",
        "\n",
        "\n",
        "           = 0.25\n",
        "\n",
        "\n",
        "For Age = 40:\n",
        "\n",
        "Age_scaled = (40 - 20) / 40\n",
        "           \n",
        "           \n",
        "           = 20 / 40\n",
        "           = 0.5\n",
        "\n",
        "\n",
        "For Age = 50\n",
        "\n",
        ":Age_scaled = (50 - 20) / 40\n",
        "\n",
        "\n",
        "           = 30 / 40\n",
        "\n",
        "\n",
        "           = 0.75\n",
        "\n",
        "\n",
        "\n",
        "For Age = 60\n",
        "\n",
        ":Age_scaled = (60 - 20) / 40\n",
        "\n",
        "\n",
        "           = 40 / 40\n",
        "          \n",
        "          \n",
        "           = 1\n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "\n",
        "Age     Age_scaled\n",
        "\n",
        "20      0.0\n",
        "\n",
        "30      0.25\n",
        "\n",
        "40      0.5\n",
        "\n",
        "50      0.75\n",
        "\n",
        "60      1.0\n",
        "\n"
      ],
      "metadata": {
        "id": "-SMaHMD9rb4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.2  What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**"
      ],
      "metadata": {
        "id": "CGoBVGeOsXmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Unit Vector technique, also known as vector normalization or feature scaling, is a data preprocessing technique used to scale individual feature vectors to have a unit norm. This means that after scaling, each feature vector will have a Euclidean norm (magnitude) of 1.\n",
        "\n",
        "In contrast to Min-Max scaling, which scales features to a fixed range (e.g., [0, 1]), Unit Vector scaling focuses on scaling each feature vector independently, ensuring that the magnitude of each vector is consistent. This can be particularly useful when the direction of the feature vector is more important than its magnitude, such as in algorithms that rely on distance calculations or dot products, like K-nearest neighbors (KNN) or support vector machines (SVM).\n",
        "\n",
        "The Unit Vector scaling technique can be represented as follows:\n",
        "X_unit_scaled = X / ||X||\n",
        "\n",
        "Where:\n",
        "\n",
        "X is the original feature vector.\n",
        "X_unit_scaled is the rescaled feature vector with a unit norm.\n",
        "||X|| represents the Euclidean norm (magnitude) of the feature vector X.\n",
        "Here's an example to illustrate the application of Unit Vector scaling:\n",
        "\n",
        "Suppose we have a dataset with two numerical features represented by the vectors [3, 4] and [1, -2]. We want to scale these feature vectors using the Unit Vector technique.\n",
        "\n",
        "\n",
        "Original feature vectors:\n",
        "\n",
        "Feature 1: [3, 4]\n",
        "\n",
        "\n",
        "Feature 2: [1, -2]\n",
        "\n",
        "\n",
        "Calculate the Euclidean norm (magnitude) of each feature vector:\n",
        "\n",
        "For Feature 1: ||[3, 4]|| = sqrt(3^2 + 4^2) = sqrt(9 + 16) = sqrt(25) = 5\n",
        "\n",
        "For Feature 2: ||[1, -2]|| = sqrt(1^2 + (-2)^2) = sqrt(1 + 4) = sqrt(5)\n",
        "\n",
        "\n",
        "Scale each feature vector to have a unit norm:\n",
        "\n",
        "\n",
        "For Feature 1:\n",
        "\n",
        "[3, 4]_unit_scaled = [3/5, 4/5]\n",
        "                   = [0.6, 0.8]\n",
        "\n",
        "\n",
        "\n",
        "[1, -2]_unit_scaled = [1/sqrt(5), -2/sqrt(5)]\n",
        "                    â‰ˆ [0.45, -0.89]\n",
        "\n",
        "\n",
        "Scaled feature vectors with a unit norm:\n",
        "\n",
        "Feature 1: [0.6, 0.8]\n",
        "Feature 2: [0.45, -0.89]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now, each feature vector has been scaled to have a unit norm, ensuring that the direction of the vectors remains consistent while the magnitude is normalized. This can be beneficial for machine learning algorithms that rely on feature vectors' direction, such as KNN or SVM."
      ],
      "metadata": {
        "id": "r48D2lVzsqjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**"
      ],
      "metadata": {
        "id": "zXeyXh4mtsGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving the most important information in the data. It achieves this by identifying the principal components, which are new axes that capture the maximum variance in the data.\n",
        "\n",
        "PCA works by finding the eigenvectors (principal components) of the covariance matrix of the data and projecting the data onto these eigenvectors. The principal components are ordered by the amount of variance they capture, with the first principal component capturing the most variance and subsequent components capturing decreasing amounts of variance.\n",
        "\n",
        "PCA is commonly used for the following purposes:\n",
        "\n",
        "Dimensionality Reduction: PCA can reduce the number of features (dimensions) in a dataset while retaining most of the variance. This can help simplify the data and improve computational efficiency, especially in cases where there are many correlated features.\n",
        "\n",
        "Data Visualization: PCA can be used to visualize high-dimensional data in a lower-dimensional space, making it easier to interpret and analyze. By projecting the data onto a few principal components, complex relationships between variables can be visualized in two or three dimensions.\n",
        "\n",
        "Noise Reduction: PCA can also be used to denoise data by removing components with low variance, which are often associated with noise or irrelevant information.\n",
        "\n",
        "Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
        "\n",
        "Suppose we have a dataset with 1000 samples and 10 numerical features. We want to reduce the dimensionality of this dataset using PCA.\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Standardize the features to have zero mean and unit variance to ensure that each feature contributes equally to the PCA.\n",
        "Perform PCA:\n",
        "\n",
        "Compute the covariance matrix of the standardized data.\n",
        "Find the eigenvectors (principal components) and eigenvalues of the covariance matrix.\n",
        "Sort the eigenvectors by their corresponding eigenvalues in descending order to identify the principal components.\n",
        "Choose the Number of Components:\n",
        "\n",
        "Decide on the number of principal components to retain based on the cumulative explained variance. Typically, a threshold (e.g., 90% variance explained) is chosen to determine the number of components.\n",
        "Project the Data:\n",
        "\n",
        "Project the standardized data onto the selected principal components to obtain the reduced-dimensional representation of the data.\n",
        "Interpret the Results:\n",
        "\n",
        "Analyze the reduced-dimensional data to understand the underlying structure and relationships between variables.\n",
        "By applying PCA, we can reduce the dimensionality of the dataset while retaining most of the variance, allowing for more efficient analysis and visualization of the data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YVnj1XO4tsJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
        "Extraction? Provide an example to illustrate this concept.**"
      ],
      "metadata": {
        "id": "AwtbncqQtsLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q4c9pVjdwOA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_CrEfa9ZwODg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
        "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
        "preprocess the data.**"
      ],
      "metadata": {
        "id": "T78KyhFrtsNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dWMrYXJptsPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Azb2xRmWtsSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S9rK1e4JtsWj"
      }
    }
  ]
}